{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Twitter Data for Cities that Have Declared a Climate Emergency #\n",
    "\n",
    "This notebook focuses on getting Twitter data (all tweets) from the 10 largest cities (population-wise) that have declared a climate emergency. \n",
    "\n",
    "    -Los Angeles\n",
    "    -Seattle \n",
    "    -Denver\n",
    "    -New York\n",
    "    -Chicago\n",
    "    -San Diego\n",
    "    -San Jose\n",
    "    -Austin\n",
    "    -San Francisco\n",
    "    -Boston\n",
    "\n",
    "This first portion of the notebook is dedicated to printing an output that we'll use for a library called \"Twitterscraper.\" This package uses CL for data collection. We'll load in the data back into this notebook. \n",
    "\n",
    "https://github.com/taspinar/twitterscraper\n",
    "    \n",
    "Once the data from twitterscraper is loaded, for the last portion, we'll then merge all of the cities' data into one large dataset for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import json      # library for working with JSON-formatted text strings\n",
    "import pprint as pp    # library for cleanly printing Python data structures\n",
    "import seaborn as sns\n",
    "import twitterscraper as ts\n",
    "from twitterscraper import query_tweets #library downloaded\n",
    "import os as os\n",
    "\n",
    "import subprocess #this enables us to pass CL code directly from Jupyter Notebooks \n",
    "from subprocess import Popen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Twitterscraper Command ## \n",
    "\n",
    "The code below scrapes Twitter accounts from each city, scrapes *all* of their tweets, and makes one big JSON file. Rather than pasting the command into the CL, this function uses \"subprocess\" (a standard library already with Python) to pass the command directly through Jupyter Notebooks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(*json_files):\n",
    "    data_frames = []\n",
    "    \n",
    "    for file in json_files:\n",
    "        with open(json_file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        d = {'username': [x['username'] for x in data],\n",
    "        'time': [x['timestamp'] for x in data],\n",
    "        'tweet': [x['text'] for x in data],\n",
    "        'likes': [x['likes'] for x in data],\n",
    "        'replies': [x['replies'] for x in data],\n",
    "        'user_ID' : [x['screen_name'] for x in data]}\n",
    "    \n",
    "        data_frames.append(pd.DataFrame.from_dict(d))\n",
    "    return data_frames\n",
    "\n",
    "def combine_data(*data_frames): #this will allow us to merge dataframes \"*\" allows us to pass X dataframes\n",
    "    return pd.concat(data_frames)\n",
    "\n",
    "def buildQuery(accounts):\n",
    "    scraper_query = ''\n",
    "    \n",
    "    #this builds our search query\n",
    "    for index, each_account in enumerate (accounts):\n",
    "        next_index = index + 1 #this is so that we don't have an extra \"OR\" at the end, it \"knows\" the last thing\n",
    "        if next_index > len(accounts) - 1: \n",
    "            scraper_query = scraper_query + \"from:\"+ each_account\n",
    "        else:\n",
    "            scraper_query = scraper_query + \"from:\"+ each_account + \" OR \"\n",
    "            \n",
    "    return scraper_query\n",
    "\n",
    "def launch(command, output):\n",
    "    print (command)\n",
    "    \n",
    "    outputFile = open(output, 'w+')\n",
    "    p = Popen(command, stdout=outputFile, stderr=outputFile, universal_newlines=True)\n",
    "    output, errors = p.communicate()\n",
    "    #p.wait() # Wait for sub process to finish before moving on to make frame \n",
    "    \n",
    "    if errors:\n",
    "        print (errors)\n",
    "    myoutput.close()\n",
    "            \n",
    "def scrape(accounts):\n",
    "    data_files = []\n",
    "    \n",
    "    for user in accounts:\n",
    "        path_to_output_file = user + \".txt\" #we'll get both txt and json, but just ignore txt\n",
    "        path_to_data_file = user + \".JSON\"\n",
    "        data_files.append(path_to_data_file)\n",
    "        \n",
    "        query = 'from: ' + user\n",
    "        command = [\"twitterscraper\", query, \n",
    "                   \"--lang\", \"en\", \"--all\", \"-ow\", \"-p\", \"40\", \"-o\", path_to_data_file]\n",
    "        launch(command, path_to_output_file)\n",
    " \n",
    "\n",
    "    #twitterscraper_query = buildQuery(accounts)\n",
    "    #command = [\"twitterscraper\", twitterscraper_query, \"--lang\", \"en\", \"-o\", path_to_data_file, \"--all\", \"-ow\", \"-p\", \"40\"]\n",
    "    #launch(command, path_to_output_file)\n",
    "    \n",
    "    return data_files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "King County Metro üöè üöåüöé‚õ¥üöê                              2921\n",
       "seattledot                                            2884\n",
       "LA Metro                                              2808\n",
       "NYC DOT                                               1270\n",
       "NYC Parks                                             1075\n",
       "NYCHA                                                  678\n",
       "MTA. Stay Home. Stop the Spread.                       511\n",
       "NYC Emergency Management                               332\n",
       "Los Angeles County Parks & Recreation                  228\n",
       "LADOT                                                  200\n",
       "Port of Los Angeles                                    152\n",
       "Seattle Office of Planning & Community Development     114\n",
       "NYCPlanning                                             80\n",
       "City of Seattle                                         72\n",
       "Seattle OSE                                             63\n",
       "City of Los Angeles                                     62\n",
       "Los Angeles City Planning                               37\n",
       "HCIDLA                                                  16\n",
       "Name: username, dtype: int64"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "climate_emergency_frame = json_to_df(climate_emergency_output)\n",
    "climate_emergency_frame['username'].value_counts()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
