{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Twitter Data for other Users #\n",
    "\n",
    "This notebook focuses on getting Twitter data (all tweets) from influential users. \n",
    "\n",
    "    -Leonardo Di Caprio\n",
    "    -Cato Institute\n",
    "    -Greenpeace\n",
    "   \n",
    "\n",
    "This first portion of the notebook is dedicated to printing an output that we'll use for a library called \"Twitterscraper.\" This package uses CL for data collection. We'll load in the data back into this notebook. \n",
    "\n",
    "https://github.com/taspinar/twitterscraper\n",
    "    \n",
    "Once the data from twitterscraper is loaded, for the last portion, we'll then merge all of their data into one large dataset for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import json      # library for working with JSON-formatted text strings\n",
    "import pprint as pp    # library for cleanly printing Python data structures\n",
    "import seaborn as sns\n",
    "import twitterscraper as ts\n",
    "from twitterscraper import query_tweets #library downloaded\n",
    "import os as os\n",
    "\n",
    "import subprocess #this enables us to pass CL code directly from Jupyter Notebooks \n",
    "from subprocess import Popen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Twitterscraper Command ## \n",
    "\n",
    "The code below scrapes Twitter accounts from each city, scrapes *all* of their tweets, and makes one big JSON file. Rather than pasting the command into the CL, this function uses \"subprocess\" (a standard library already with Python) to pass the command directly through Jupyter Notebooks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(json_files):\n",
    "    data_frames = []\n",
    "    \n",
    "    for file in json_files:\n",
    "        print (file)\n",
    "        with open(file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        d = {'username': [x['username'] for x in data],\n",
    "        'time': [x['timestamp'] for x in data],\n",
    "        'tweet': [x['text'] for x in data],\n",
    "        'likes': [x['likes'] for x in data],\n",
    "        'replies': [x['replies'] for x in data],\n",
    "        'user_ID' : [x['screen_name'] for x in data]}\n",
    "    \n",
    "        data_frames.append(pd.DataFrame.from_dict(d))\n",
    "    return data_frames\n",
    "\n",
    "def combine_data(data_frames): #this will allow us to merge dataframes \"*\" allows us to pass X dataframes\n",
    "    return pd.concat(data_frames)\n",
    "\n",
    "\n",
    "def buildQuery(accounts):\n",
    "    scraper_query = ''\n",
    "    \n",
    "    #this builds our search query\n",
    "    for index, each_account in enumerate (accounts):\n",
    "        next_index = index + 1 #this is so that we don't have an extra \"OR\" at the end, it \"knows\" the last thing\n",
    "        if next_index > len(accounts) - 1: \n",
    "            scraper_query = scraper_query + \"from:\"+ each_account\n",
    "        else:\n",
    "            scraper_query = scraper_query + \"from:\"+ each_account + \" OR \"\n",
    "            \n",
    "    return scraper_query\n",
    "\n",
    "def launch(command, output):\n",
    "    print (command)\n",
    "    \n",
    "    outputFile = open(output, 'w+')\n",
    "    p = Popen(command, stdout=outputFile, stderr=outputFile, universal_newlines=True)\n",
    "    output, errors = p.communicate()\n",
    "    #p.wait() # Wait for sub process to finish before moving on to make frame \n",
    "    \n",
    "    if errors:\n",
    "        print (errors)\n",
    "    outputFile.close()\n",
    "            \n",
    "def scrape(accounts):\n",
    "    data_files = []\n",
    "    \n",
    "    for user in accounts:\n",
    "        path_to_output_file = user + \".txt\" #we'll get both txt and json, but just ignore txt\n",
    "        path_to_data_file = user + \".JSON\"\n",
    "        data_files.append(path_to_data_file)\n",
    "        \n",
    "        query = 'from: ' + user\n",
    "        command = [\"twitterscraper\", query, \n",
    "                   \"--lang\", \"en\", \"--all\", \"-ow\", \"-p\", \"40\", \"-o\", path_to_data_file]\n",
    "        launch(command, path_to_output_file)\n",
    " \n",
    "    return data_files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I created a list of all the accounts I wish to scrape (I broke it up into 3 \"searches\" because this process is extremely time-consuming). However, using \"scrape()\" you can input all the accounts, it'll just an hour or so to get all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['twitterscraper', 'from: LeoDiCaprio', '--lang', 'en', '--all', '-ow', '-p', '40', '-o', 'LeoDiCaprio.JSON']\n",
      "['twitterscraper', 'from: Greenpeace', '--lang', 'en', '--all', '-ow', '-p', '40', '-o', 'Greenpeace.JSON']\n",
      "['twitterscraper', 'from: CatoInstitute', '--lang', 'en', '--all', '-ow', '-p', '40', '-o', 'CatoInstitute.JSON']\n"
     ]
    }
   ],
   "source": [
    "comp_accounts = [\"LeoDiCaprio\", \"Greenpeace\", \"CatoInstitute\"]\n",
    "                             \n",
    "comp_accounts_output = scrape(comp_accounts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting JSONs to DataFrames ##\n",
    "\n",
    "json_to_df() takes the json list output above and converts all the data into a list of dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeoDiCaprio.JSON\n",
      "Greenpeace.JSON\n",
      "CatoInstitute.JSON\n"
     ]
    }
   ],
   "source": [
    "dataframe_1 = json_to_df(comp_accounts_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>time</th>\n",
       "      <th>tweet</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>user_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>Leonardo DiCaprio</td>\n",
       "      <td>2011-10-21T19:52:58</td>\n",
       "      <td>#SaveTigersNow RT @World_Wildlife: Tragic imag...</td>\n",
       "      <td>325</td>\n",
       "      <td>164</td>\n",
       "      <td>LeoDiCaprio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Greenpeace</td>\n",
       "      <td>2008-12-17T15:35:14</td>\n",
       "      <td>@creativemuffin I like talking to people who w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Greenpeace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Greenpeace</td>\n",
       "      <td>2015-05-01T16:04:05</td>\n",
       "      <td>Investing in fossil fuels means you profit fro...</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>Greenpeace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Greenpeace</td>\n",
       "      <td>2018-07-03T22:26:58</td>\n",
       "      <td>For the past 8 hours, 12 climbers have stopped...</td>\n",
       "      <td>510</td>\n",
       "      <td>10</td>\n",
       "      <td>Greenpeace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>Greenpeace</td>\n",
       "      <td>2009-09-08T13:37:24</td>\n",
       "      <td>RT @AslihanTumer: some more photos from #antia...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Greenpeace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              username                 time  \\\n",
       "629  Leonardo DiCaprio  2011-10-21T19:52:58   \n",
       "23          Greenpeace  2008-12-17T15:35:14   \n",
       "140         Greenpeace  2015-05-01T16:04:05   \n",
       "163         Greenpeace  2018-07-03T22:26:58   \n",
       "288         Greenpeace  2009-09-08T13:37:24   \n",
       "\n",
       "                                                 tweet  likes  replies  \\\n",
       "629  #SaveTigersNow RT @World_Wildlife: Tragic imag...    325      164   \n",
       "23   @creativemuffin I like talking to people who w...      0        0   \n",
       "140  Investing in fossil fuels means you profit fro...     43        2   \n",
       "163  For the past 8 hours, 12 climbers have stopped...    510       10   \n",
       "288  RT @AslihanTumer: some more photos from #antia...      0        0   \n",
       "\n",
       "         user_ID  \n",
       "629  LeoDiCaprio  \n",
       "23    Greenpeace  \n",
       "140   Greenpeace  \n",
       "163   Greenpeace  \n",
       "288   Greenpeace  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge1 = combine_data(dataframe_1)\n",
    "#merge2 = combine_data(dataframe_2)\n",
    "#merge3 = combine_data(dataframe_3)\n",
    "\n",
    "frames = [merge1]\n",
    "\n",
    "result = pd.concat(frames)\n",
    "result\n",
    "\n",
    "to_keep = [\"LeoDiCaprio\", \"Greenpeace\", \"CatoInstitute\"]\n",
    "\n",
    "final_results = result[~result['user_ID'].isin(to_keep) == False] # the code above got all mentions & replies\n",
    "final_results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.to_csv(\"Other Users Results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
